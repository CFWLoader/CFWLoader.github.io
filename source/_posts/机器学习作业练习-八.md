---
title: 机器学习作业练习(八)
date: 2018-08-05 16:08:05
tags:
    - 机器学习
    - 作业练习
    - R
    - 统计理论
    - VC维
categories: 机器学习
---

本章节是一些统计学习理论。

### 题目8.1 - Vapnik-Chervonenkis dimension，VC维

使用$\tilde{C}_{p,N}$的定义以及二项式的递归属性：

$$
(x+y)^n = \sum_{k=0}^{n}\left(\begin{matrix}n \\ k\end{matrix}\right)x^{n-k}y^k
$$

对于$x,y\in\mathbb{R},n\in\mathbb{N}$，证明一个线性分类器，$y(x;w)=sign(w_0 + \sum_{i=1}^{N}x_i w_i)$，其VC维$d_{VC}=N+1$。

记住，$$\tilde{C}_{(N+1,N)} = 2^{N+1}$$以及$$\tilde{C}_{(N+2,N)}<2^{N+2}$$。

显然笔者暂时不会证，或者网上已经有大堆博客，这题先放着。

### 题目8.2 - 分类可变性

假设样本$x^{(\alpha)}\in\mathbb{R}^2$分别属于两个聚类$C_1, C_2$，分别通过正态分布$\mathcal{N}(\mu_i, 2I),i=1,2$，其中$\mu_1 = (0,1)^T, \mu_2 = (1,0)^T$，$I$是单位阵。使用N个样本，测试一个线性神经元学习出来的模型的性能如何。按照如下步骤:

1. 对于两个聚类，分别产生$N/2$个数据点$x^{(\alpha)}$，当类属为$C_1$时，$y^{(\alpha)}=1$；当，当类属为$C_2$时，$y^{(\alpha)}=-1$。
2. 根据均方误差最小化，得出$y(x)=sign(w_0+\sum_{i=1}^{N}w_i x_i)$。
3. 产生根据同样的分布，每个类属产生500个测试样本。
4. 分别计算经验（训练）误差以及测试误差。

对于$N\in \{2,4,6,8,10,20,40,100\}$，分别重复50次，记下训练后的参数以及训练集的精度和测试集的精度。

第一问使用误差棒图描绘出每个N下的训练精度以及测试集精度均值和标准差：

![](机器学习作业练习-八/e8-2a-all.png)

可以看到训练样本越少，模型越容易过拟合，泛化性能越差，模型也不稳定（方差大）。

第二问分别绘出$w_0, w_1, w_2$关于N的误差棒图：

![](机器学习作业练习-八/e8-2b.png)

基本情况跟第一问一样，样本越少，重复中出来的参数误差越大。

（未完待续）