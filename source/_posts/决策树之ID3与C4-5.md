---
title: 决策树之ID3与C4.5
date: 2018-10-17 20:37:48
tags:
    - 分类树
    - ID3
    - C4.5
    - 信息增益
    - 信息增益比
categories: 数据挖掘
---

笔者最近开始学习李航的《统计学习方法》，将以前零散学习的数据挖掘/机器学习算法系统整理一遍，而决策树则可以说是一个入门级的内容了。

李航将机器学习方法等价为：模型+策略+算法，笔者认为这个总结十分受用，便于拆解一些经典的算法逐步理解。这种自上而下将任务拆解的方法则很合适工程类人士学习。

根据李航给出的框架，首先将决策树分解为：
``` text 
模型：决策树，归根结底就是一个条件概率分布函数。
策略：选择模型的准则，在决策树中则为决策树的损失函数。
算法：求解决策树的算法，在此是生成决策树的算法。
```

决策树学习的三个步骤：特征选择、决策树生成、决策树剪枝。

## 模型

在此语境下，模型即是决策树。分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。

## 策略

在生成决策树时是不需要损失函数的，但是不经过剪枝的决策树会有过于复杂以及过拟合的情况，而剪枝在某种程度上解决该问题，于是便在生成决策树后进行一个剪枝的操作，而剪枝所用的损失函数：

$$
\begin{align}
C_{\alpha}=\sum_{t=1}^{T}N_t H_t(T)+\alpha |T| \\
H_t(T)=-\sum_{k}\frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}
\end{align}
$$

其中，$|T|$表示决策树叶子节点个数，$$N_t$$表示叶子节点下有$$N_t$$个样本，其中第$k$类样本点$N_{tk}$个。$\alpha$参数影响惩罚力度，$\alpha$越大则对叶子节点数量越敏感，此时算法倾向于选择叶子节点少的决策树。

## 算法

生成一颗决策树核心的问题是，如何选取特征作为分裂当前节点的依据？

（未完待续）