---
title: 逻辑斯谛回归模型
date: 2018-12-10 11:01:17
tags:
    - 统计学
    - 机器学习
    - 概率模型
    - Logsitic分布
categories: 机器学习
---

逻辑斯谛回归模型（Logistic Regression，以下简称LR）是统计学习中的经典分类方法。读过《统计学习方法》之后，笔者才知道该模型是一种`概率分布模型`，从此理解为何该模型不但可以预测分类，还可以预测属于该类的概率。

## 逻辑斯谛分布

设$X$是连续随机变量，若$X$服从逻辑斯谛分布，则有如下分布函数和密度函数：

$$
\begin{aligned}
F(x) = P(X \leq x) & = \frac{1}{1 + e^{-(x-\mu)/\gamma}} \\
f(x) = F\prime (x) & = \frac{e^{-(x-\mu)/\gamma}}{\gamma(1 + e^{-(x-\mu)/\gamma})^2}
\end{aligned}
$$

下面两张是$\mu = 1.1$时的概率密度分布以及累计概率分布：

![](逻辑斯谛回归模型/logistic-dist.jpg)

可以看到PDF关于Y轴对称，CDF关于$(\mu, \frac{1}{2})$对称。

曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数$\gamma$越小，曲线在中心附近增长越快。

## 二项逻辑斯谛回归模型

二项逻辑斯谛回归模型（Binomial Logistic Regression Model）是一种`分类模型`，由条件概率分布$P(Y|X)$表示，随机变量$Y$取值0或1，$X\in \mathbb{R}^n$。

故二项逻辑斯谛回归模型是如下的条件概率分布：

$$
\begin{aligned}
P(Y=1|x) & = \frac{\exp(w\cdot x + b)}{1 + \exp(w\cdot x + b)} \\
P(Y=0|x) & = \frac{1}{1 + \exp(w\cdot x + b)}
\end{aligned}
$$

### 逻辑斯谛回归模型的特点

定义一个事件的`几率（odds）`是指该事件发生的概率与该事件不发生的概率的比值。设事件发生的概率为$p$，则该事件的几率是$\frac{p}{1-p}$，事件的`对数几率（logit odds）`或logit函数：

$$
logit(p)=\log\frac{p}{1-p}
$$

将上述的$P(Y=1|x)$代入到$p$中，则有：

$$
logit(p)=\log\frac{p}{1-p}=\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=\log\frac{\frac{\exp(w\cdot x + b)}{1 + \exp(w\cdot x + b)}}{1-\frac{\exp(w\cdot x + b)}{1 + \exp(w\cdot x + b)}}=w\cdot x + b
$$

即，在逻辑斯谛回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数。

从另外一个角度看，考虑对输入$x$进行分类的线性函数$w\cdot x$，其值域为实数域。此时，线性函数的值越接近于正无穷，概率值越接近1；线性函数的值越接近负无穷，概率值越接近0.

## 模型参数估计

利用[极大似然估计法](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)可以估计模型的参数，设：

$$
P(Y=1|x)=\pi(x),P(Y=0|x)=1-\pi(x)
$$

似然函数：

$$
\prod_{i=1}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$

对数似然函数：

$$
\begin{aligned}
L(w)&=\sum_{i=1}^{N}[y_i \log\pi(x_i) + (1-y_i)\log(1-\pi(x_i))] \\
&=\sum_{i=1}^{N}[y_i \log\frac{\pi(x_i)}{1-\pi(x_i)} + \log(1-\pi(x_i))] \\
&=\sum_{i=1}^{N}[y_i(w\cdot x_i) - \log(1+\exp(w \cdot x_i))]
\end{aligned}
$$

问题就转变为了以对数似然函数为目标函数的最优化问题。逻辑斯谛回归学习中通常采用梯度下降法或拟牛顿法。

对权重向量求偏导则可得：

$$
\frac{\partial L(w)}{\partial w} = x_i (y_i - \frac{\exp(w \cdot x_i)}{1+\exp(w \cdot x_i)})
$$

采用$sigmoid(x)=\frac{\exp(x)}{1+\exp(x)}$，则偏导可以表示为：

$$
\frac{\partial L(w)}{\partial w} = x_i (y_i - sigmoid(w\cdot x_i))
$$

这篇文章仅仅介绍逻辑斯谛回归模型，不介绍最优化常用的方法，读者可以寻找各类最优化方法，将上述提到的目标函数以及一阶偏导代入到各类优化方法中。